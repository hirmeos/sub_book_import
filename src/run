#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Take a CSV file containing title and URIs associated to a DOI and submit new
findings to the identifier translation service.

usage: python run

(c) Javier Arias, Open Book Publishers, October 2018
Use of this software is governed by the terms of the MIT license

Dependencies:
  httplib2==0.10.3
  pandas==0.23.4
"""

import os
import sys
import json
import httplib2
import pandas as pd
from urlparse import urlparse

CSV_PATH          = os.environ['CSV_PATH']
DEFAULT_TYPE      = os.environ['DEFAULT_TYPE']
URI_API_ENDP      = os.environ['URI_API_ENDP']
URI_API_USER      = os.environ['URI_API_USER']
URI_API_PASS      = os.environ['URI_API_PASS']
AUTH_API_ENDP     = os.environ['AUTH_API_ENDP']
URI_API_WORKS     = os.environ['URI_API_WORKS']
URI_API_URIS      = os.environ['URI_API_URIS']
URI_API_TITLES    = os.environ['URI_API_TITLES']


def get_token(url, email, passwd):
    h = httplib2.Http()
    credentials = {'email': email, 'password': passwd}
    headers = {'content-type': 'application/json'}
    res, content = h.request(url, 'POST', json.dumps(credentials), headers)
    try:
        assert res.status == 200
    except AssertionError:
        raise ValueError(content.decode('utf-8'))
    return json.loads(content.decode('utf-8'))['data'][0]['token']


def get_from_doi(doi):
    url = URI_API_ENDP + "?uri=" + doi
    h = httplib2.Http()
    headers = {'Authorization': 'Bearer ' + API_JWTOKEN}
    res, content = h.request(url, 'GET', headers=headers)
    return json.loads(content.decode('utf-8'))['data']


def is_doi_unique(doi):
    url = '%s?uri=%s&filter=uri_scheme:info:doi&strict=true' % (URI_API_ENDP,
                                                                doi)
    h = httplib2.Http()
    headers = {'Authorization': 'Bearer ' + API_JWTOKEN}
    res, content = h.request(url, 'GET', headers=headers)
    try:
        assert res.status == 200
    except AssertionError:
        r = json.loads(content.decode('utf-8'))
        m = "%s: %s" % (r['message'], r['parameters']['uri'])
        if r['message'] != 'No records have matched your search criteria.':
            print >>sys.stderr, m
            return False
    return True


def submit(url, data):
    h = httplib2.Http()
    headers = {'content-type': 'application/json',
               'Authorization': 'Bearer ' + API_JWTOKEN}
    res, content = h.request(url, 'POST', json.dumps(data), headers)
    try:
        assert res.status == 200
    except AssertionError:
        sys.stderr.write(content.decode('utf-8'))
        sys.exit(1)


def get_uuid_from_uris(uris):
    uuid = ''
    for uri in uris:
        assert uuid == '' or uuid == uri['work']['UUID']
        uuid = uri['work']['UUID']
    return uuid


def get_uri_from_uris(uri, uris):
    for e in uris:
        if e['URI'] == uri:
            return e['URI']
    return ''


def standarise_uri(uri, canonical):
    return {'URI': uri, 'canonical': canonical}


def standarise_uris(uris, is_canonical):
    out = []
    for val in uris:
        out.append(standarise_uri(val, is_canonical))
    return out


def preprocess_csv(mapping_file):
    df = pd.read_csv(mapping_file, encoding="utf-8", header=0, sep='|')
    df_new = pd.DataFrame()

    df_new['Title short'] = df['Title']
    df_new['ISBN-10'] = ['urn:isbn:' + str(x).strip(' ') if not x == '  ' else x for x in df['ISBN-10']]
    df_new['ISBN-13'] = ['urn:isbn:' + str(x).strip(' ') if not x == '  ' else x for x in df['ISBN-13']]
    df_new['DOI'] = ['info:doi:' + str(x).strip(' ') if not x == ' ' else x for x in df['DOI']]

    # URL different versions
    df_new['URL'] = [str(x).strip(' ') for x in df['URL']]
    df_new['alt URL'] = [str(x).strip(' ') for x in df['MORE URL']]

    df_new['URL www'] = [str(x).replace('https://', 'https://www.') for x in df_new['URL']]
    df_new['alt URL www'] = [str(x).replace('https://', 'https://www.') for x in df_new['alt URL']]

    df_new['URL http'] = [str(x).strip(' ').replace('https', 'http') for x in df_new['URL']]
    df_new['alt URL http'] = [str(x).strip(' ').replace('https', 'http') for x in df_new['alt URL']]

    df_new['URL http www'] = [str(x).strip(' ').replace('https://', 'http://www.') for x in df_new['URL']]
    df_new['alt URL http www'] = [str(x).strip(' ').replace('https://', 'http://www.') for x in df_new['alt URL']]

    # pdfview different versions
    df_new['pdfview'] = [str(x).strip(' ') for x in df['PDFVIEW']]
    df_new['alt pdfview'] = [str(x).strip(' ') for x in df['MORE PDFVIEW']]

    df_new['pdfview www'] = [str(x).replace('https://', 'https://www.') for x in df_new['pdfview']]
    df_new['alt pdfview www'] = [str(x).replace('https://', 'https://www.') for x in df_new['alt pdfview']]

    df_new['pdfview http'] = [str(x).strip(' ').replace('https', 'http') for x in df_new['pdfview']]
    df_new['alt pdfview http'] = [str(x).strip(' ').replace('https', 'http') for x in df_new['alt pdfview']]

    df_new['pdfview http www'] = [str(x).strip(' ').replace('https://', 'http://www.') for x in df_new['pdfview']]
    df_new['alt pdfview http www'] = [str(x).strip(' ').replace('https://', 'http://www.') for x in df_new['alt pdfview']]

    # hypothesis different versions, standard including https://www.
    df_new['hypothesis www'] = [str(x).strip(' ') for x in df['HYPOTHES.IS']]
    df_new['alt hypothesis www'] = [str(x).strip(' ') for x in df['HYPOTHES.IS']]

    df_new['hypothesis'] = [str(x).replace('https://wwww.', 'https://') for x in df_new['hypothesis www']]
    df_new['alt hypothesis'] = [str(x).replace('https://www.', 'https://') for x in df_new['alt hypothesis www']]

    df_new['hypothesis http'] = [str(x).strip(' ').replace('https://www.', 'http://') for x in df_new['hypothesis www']]
    df_new['alt hypothesis http'] = [str(x).strip(' ').replace('https://www.', 'http://') for x in df_new['alt hypothesis www']]

    df_new['hypothesis http www'] = [str(x).strip(' ').replace('https', 'http') for x in df_new['hypothesis www']]
    df_new['alt hypothesis http www'] = [str(x).strip(' ').replace('https', 'http') for x in df_new['alt hypothesis www']]

    df_new['URN'] = [str(x).strip(' ') for x in df['URN']]

    #make landing page links 
    df_new['url landing page'] = ['https://univerlag.uni-goettingen.de/handle/3/isbn-'str(x).strip(' ') for x in df['ISBN-13']]


    return df_new


def is_uri(uri):
    return isinstance(uri, str) and bool(urlparse(uri).scheme)


def process_csv(mapping_file):
    df = preprocess_csv(mapping_file)

    for row in df.index:
        title = df.at[row, 'Title short']

        doi = df.at[row, 'DOI']
        # isbn_10 = df.at[row, 'ISBN-10']
        # isbn_13 = df.at[row, 'ISBN-13']

        uris = []
        # for x in [df.at[row, 'URL'], df.at[row, 'alt URL'], df.at[row, 'URN'], isbn_10, isbn_13]:
        for col in df.columns:
            if not col == 'Title short' or not col == "DOI":
                x = df.at[row, col]
            if is_uri(x):
                uris.append(x)

        input_uris = standarise_uris(uris, False)
        input_uris.append(standarise_uri(doi, True))

        try:
            if not title or not is_uri(doi):
                raise AssertionError
            if not is_doi_unique(doi):
                raise AssertionError
        except AssertionError:
            continue

        try:
            uris = get_from_doi(doi)
            uuid = get_uuid_from_uris(uris)
            assert uris and uuid
        except AssertionError:
            # add new work
            work = {'title': [title], 'type': DEFAULT_TYPE, 'URI': input_uris}
            submit(URI_API_WORKS, work)
            continue

        # insert input URIs if not already in database
        for uri in input_uris:
            candidate = get_uri_from_uris(uri['URI'], uris)
            try:
                assert candidate == ''
            except AssertionError:
                continue
            new_uri = {'UUID': uuid, 'URI': uri['URI'],
                       'canonical': uri['canonical']}
            submit(URI_API_URIS, new_uri)
        # now submit the input title in case it's been updated
        submit(URI_API_TITLES, {'UUID': uuid, 'title': title})


def run():
    assert API_JWTOKEN
    process_csv(CSV_PATH)


API_JWTOKEN = get_token(AUTH_API_ENDP, URI_API_USER, URI_API_PASS)


if __name__ == '__main__':
    run()
